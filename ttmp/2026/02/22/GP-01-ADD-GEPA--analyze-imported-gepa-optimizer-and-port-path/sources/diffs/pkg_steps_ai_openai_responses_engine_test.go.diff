--- /home/manuel/workspaces/2026-02-22/add-gepa-optimizer/geppetto/pkg/steps/ai/openai_responses/engine_test.go	2026-02-22 17:02:05.269595576 -0500
+++ /home/manuel/workspaces/2026-02-22/add-gepa-optimizer/imported/geppetto-main/pkg/steps/ai/openai_responses/engine_test.go	2026-02-22 16:34:26.000000000 -0500
@@ -2,6 +2,7 @@
 
 import (
 	"context"
+	"encoding/json"
 	"io"
 	"net/http"
 	"strings"
@@ -146,7 +147,7 @@
 				`data: {"item":{"type":"message","id":"msg_1"}}`,
 				"",
 				"event: response.completed",
-				`data: {"response":{"usage":{"input_tokens":10,"output_tokens":5,"output_tokens_details":{"reasoning_tokens":3}}}}`,
+				`data: {"response":{"usage":{"input_tokens":10,"output_tokens":5,"input_tokens_details":{"cached_tokens":4},"output_tokens_details":{"reasoning_tokens":3}}}}`,
 				"",
 			}, "\n")
 			return &http.Response{
@@ -190,6 +191,7 @@
 	var thinkingPartialEvents int
 	var finalEvents int
 	var finalThinkingText string
+	var finalUsage *events.Usage
 	for _, event := range sink.snapshot() {
 		switch e := event.(type) {
 		case *events.EventReasoningTextDelta:
@@ -200,6 +202,7 @@
 			thinkingPartialEvents++
 		case *events.EventFinal:
 			finalEvents++
+			finalUsage = e.Metadata().Usage
 			if e.Metadata().Extra != nil {
 				if s, ok := e.Metadata().Extra["thinking_text"].(string); ok {
 					finalThinkingText = s
@@ -223,6 +226,12 @@
 	if finalThinkingText != "Thinking hard." {
 		t.Fatalf("expected final metadata thinking_text to be propagated, got %q", finalThinkingText)
 	}
+	if finalUsage == nil {
+		t.Fatalf("expected final usage metadata")
+	}
+	if finalUsage.InputTokens != 10 || finalUsage.OutputTokens != 5 || finalUsage.CachedTokens != 4 {
+		t.Fatalf("expected usage input=10 output=5 cached=4, got input=%d output=%d cached=%d", finalUsage.InputTokens, finalUsage.OutputTokens, finalUsage.CachedTokens)
+	}
 }
 
 func TestRunInference_StreamingReasoningTextDonePreservesAccumulatedThinking(t *testing.T) {
@@ -331,3 +340,384 @@
 		t.Fatalf("expected combined thinking_text, got %q", finalThinkingText)
 	}
 }
+
+func TestRunInference_StreamingOutputItemDoneDoesNotDuplicateStreamedText(t *testing.T) {
+	origClient := http.DefaultClient
+	http.DefaultClient = &http.Client{
+		Transport: roundTripperFunc(func(r *http.Request) (*http.Response, error) {
+			if r.Method != http.MethodPost {
+				t.Fatalf("expected POST, got %s", r.Method)
+			}
+			if r.URL.Path != "/v1/responses" {
+				t.Fatalf("unexpected path: %s", r.URL.Path)
+			}
+			body := strings.Join([]string{
+				"event: response.output_item.added",
+				`data: {"item":{"type":"message","id":"msg_1"}}`,
+				"",
+				"event: response.output_text.delta",
+				`data: {"delta":"Hel","item_id":"msg_1"}`,
+				"",
+				"event: response.output_text.delta",
+				`data: {"delta":"lo","item_id":"msg_1"}`,
+				"",
+				"event: response.output_item.done",
+				`data: {"item":{"type":"message","id":"msg_1","content":[{"type":"output_text","text":"Hello"}]}}`,
+				"",
+				"event: response.completed",
+				`data: {"response":{"usage":{"input_tokens":1,"output_tokens":1}}}`,
+				"",
+			}, "\n")
+			return &http.Response{
+				StatusCode: http.StatusOK,
+				Header:     http.Header{"Content-Type": []string{"text/event-stream"}},
+				Body:       io.NopCloser(strings.NewReader(body)),
+				Request:    r,
+			}, nil
+		}),
+	}
+	defer func() { http.DefaultClient = origClient }()
+
+	eng, err := NewEngine(&settings.StepSettings{
+		API: &settings.APISettings{
+			APIKeys:  map[string]string{"openai-api-key": "test"},
+			BaseUrls: map[string]string{"openai-base-url": "https://example.test/v1"},
+		},
+		Chat: &settings.ChatSettings{
+			Engine: ptr("gpt-5-mini"),
+			Stream: true,
+		},
+	})
+	if err != nil {
+		t.Fatalf("NewEngine: %v", err)
+	}
+
+	sink := &capturingEventSink{}
+	ctx := events.WithEventSinks(context.Background(), sink)
+	turn := &turns.Turn{Blocks: []turns.Block{
+		turns.NewSystemTextBlock("You are a LLM."),
+		turns.NewUserTextBlock("Hello"),
+	}}
+
+	_, err = eng.RunInference(ctx, turn)
+	if err != nil {
+		t.Fatalf("expected no error, got %v", err)
+	}
+
+	var finalText string
+	var partialDeltas []string
+	for _, event := range sink.snapshot() {
+		switch e := event.(type) {
+		case *events.EventPartialCompletion:
+			partialDeltas = append(partialDeltas, e.Delta)
+		case *events.EventFinal:
+			finalText = e.Text
+		}
+	}
+
+	if finalText != "Hello" {
+		t.Fatalf("expected final text %q, got %q", "Hello", finalText)
+	}
+	if strings.Join(partialDeltas, "") != "Hello" {
+		t.Fatalf("expected partial deltas to compose %q, got %q", "Hello", strings.Join(partialDeltas, ""))
+	}
+	if len(partialDeltas) != 2 {
+		t.Fatalf("expected exactly two streamed partial deltas, got %d (%v)", len(partialDeltas), partialDeltas)
+	}
+}
+
+func TestRunInference_StreamingOutputItemDoneBackfillsMissingTail(t *testing.T) {
+	origClient := http.DefaultClient
+	http.DefaultClient = &http.Client{
+		Transport: roundTripperFunc(func(r *http.Request) (*http.Response, error) {
+			if r.Method != http.MethodPost {
+				t.Fatalf("expected POST, got %s", r.Method)
+			}
+			if r.URL.Path != "/v1/responses" {
+				t.Fatalf("unexpected path: %s", r.URL.Path)
+			}
+			body := strings.Join([]string{
+				"event: response.output_item.added",
+				`data: {"item":{"type":"message","id":"msg_1"}}`,
+				"",
+				"event: response.output_text.delta",
+				`data: {"delta":"Hel","item_id":"msg_1"}`,
+				"",
+				"event: response.output_item.done",
+				`data: {"item":{"type":"message","id":"msg_1","content":[{"type":"output_text","text":"Hello"}]}}`,
+				"",
+				"event: response.completed",
+				`data: {"response":{"usage":{"input_tokens":1,"output_tokens":1}}}`,
+				"",
+			}, "\n")
+			return &http.Response{
+				StatusCode: http.StatusOK,
+				Header:     http.Header{"Content-Type": []string{"text/event-stream"}},
+				Body:       io.NopCloser(strings.NewReader(body)),
+				Request:    r,
+			}, nil
+		}),
+	}
+	defer func() { http.DefaultClient = origClient }()
+
+	eng, err := NewEngine(&settings.StepSettings{
+		API: &settings.APISettings{
+			APIKeys:  map[string]string{"openai-api-key": "test"},
+			BaseUrls: map[string]string{"openai-base-url": "https://example.test/v1"},
+		},
+		Chat: &settings.ChatSettings{
+			Engine: ptr("gpt-5-mini"),
+			Stream: true,
+		},
+	})
+	if err != nil {
+		t.Fatalf("NewEngine: %v", err)
+	}
+
+	sink := &capturingEventSink{}
+	ctx := events.WithEventSinks(context.Background(), sink)
+	turn := &turns.Turn{Blocks: []turns.Block{
+		turns.NewSystemTextBlock("You are a LLM."),
+		turns.NewUserTextBlock("Hello"),
+	}}
+
+	_, err = eng.RunInference(ctx, turn)
+	if err != nil {
+		t.Fatalf("expected no error, got %v", err)
+	}
+
+	var finalText string
+	var partialDeltas []string
+	for _, event := range sink.snapshot() {
+		switch e := event.(type) {
+		case *events.EventPartialCompletion:
+			partialDeltas = append(partialDeltas, e.Delta)
+		case *events.EventFinal:
+			finalText = e.Text
+		}
+	}
+
+	if finalText != "Hello" {
+		t.Fatalf("expected final text %q, got %q", "Hello", finalText)
+	}
+	if strings.Join(partialDeltas, "") != "Hello" {
+		t.Fatalf("expected partial deltas to compose %q, got %q", "Hello", strings.Join(partialDeltas, ""))
+	}
+	if len(partialDeltas) != 2 {
+		t.Fatalf("expected two partial deltas (stream + backfill), got %d (%v)", len(partialDeltas), partialDeltas)
+	}
+	if partialDeltas[1] != "lo" {
+		t.Fatalf("expected done backfill delta %q, got %q", "lo", partialDeltas[1])
+	}
+}
+
+func TestRunInference_StreamingPreservesWhitespaceOnlyDelta(t *testing.T) {
+	origClient := http.DefaultClient
+	http.DefaultClient = &http.Client{
+		Transport: roundTripperFunc(func(r *http.Request) (*http.Response, error) {
+			if r.Method != http.MethodPost {
+				t.Fatalf("expected POST, got %s", r.Method)
+			}
+			if r.URL.Path != "/v1/responses" {
+				t.Fatalf("unexpected path: %s", r.URL.Path)
+			}
+			body := strings.Join([]string{
+				"event: response.output_item.added",
+				`data: {"item":{"type":"message","id":"msg_1"}}`,
+				"",
+				"event: response.output_text.delta",
+				`data: {"delta":"Hello","item_id":"msg_1"}`,
+				"",
+				"event: response.output_text.delta",
+				`data: {"delta":" ","item_id":"msg_1"}`,
+				"",
+				"event: response.output_text.delta",
+				`data: {"delta":"world","item_id":"msg_1"}`,
+				"",
+				"event: response.output_item.done",
+				`data: {"item":{"type":"message","id":"msg_1","content":[{"type":"output_text","text":"Hello world"}]}}`,
+				"",
+				"event: response.completed",
+				`data: {"response":{"usage":{"input_tokens":1,"output_tokens":1}}}`,
+				"",
+			}, "\n")
+			return &http.Response{
+				StatusCode: http.StatusOK,
+				Header:     http.Header{"Content-Type": []string{"text/event-stream"}},
+				Body:       io.NopCloser(strings.NewReader(body)),
+				Request:    r,
+			}, nil
+		}),
+	}
+	defer func() { http.DefaultClient = origClient }()
+
+	eng, err := NewEngine(&settings.StepSettings{
+		API: &settings.APISettings{
+			APIKeys:  map[string]string{"openai-api-key": "test"},
+			BaseUrls: map[string]string{"openai-base-url": "https://example.test/v1"},
+		},
+		Chat: &settings.ChatSettings{
+			Engine: ptr("gpt-5-mini"),
+			Stream: true,
+		},
+	})
+	if err != nil {
+		t.Fatalf("NewEngine: %v", err)
+	}
+
+	sink := &capturingEventSink{}
+	ctx := events.WithEventSinks(context.Background(), sink)
+	turn := &turns.Turn{Blocks: []turns.Block{
+		turns.NewSystemTextBlock("You are a LLM."),
+		turns.NewUserTextBlock("Hello"),
+	}}
+
+	_, err = eng.RunInference(ctx, turn)
+	if err != nil {
+		t.Fatalf("expected no error, got %v", err)
+	}
+
+	var finalText string
+	var partialDeltas []string
+	for _, event := range sink.snapshot() {
+		switch e := event.(type) {
+		case *events.EventPartialCompletion:
+			partialDeltas = append(partialDeltas, e.Delta)
+		case *events.EventFinal:
+			finalText = e.Text
+		}
+	}
+
+	if finalText != "Hello world" {
+		t.Fatalf("expected final text %q, got %q", "Hello world", finalText)
+	}
+	if strings.Join(partialDeltas, "") != "Hello world" {
+		t.Fatalf("expected partial deltas to compose %q, got %q", "Hello world", strings.Join(partialDeltas, ""))
+	}
+	if len(partialDeltas) != 3 {
+		t.Fatalf("expected exactly three streamed partial deltas, got %d (%v)", len(partialDeltas), partialDeltas)
+	}
+	if partialDeltas[1] != " " {
+		t.Fatalf("expected preserved whitespace-only delta %q, got %q", " ", partialDeltas[1])
+	}
+}
+
+func TestRunInference_NonStreamingUsageIncludesCachedTokens(t *testing.T) {
+	origClient := http.DefaultClient
+	http.DefaultClient = &http.Client{
+		Transport: roundTripperFunc(func(r *http.Request) (*http.Response, error) {
+			if r.Method != http.MethodPost {
+				t.Fatalf("expected POST, got %s", r.Method)
+			}
+			if r.URL.Path != "/v1/responses" {
+				t.Fatalf("unexpected path: %s", r.URL.Path)
+			}
+			body := `{
+  "output": [
+    {
+      "type": "message",
+      "id": "msg_1",
+      "content": [
+        {"type": "output_text", "text": "Done"}
+      ]
+    }
+  ],
+  "usage": {
+    "input_tokens": 12,
+    "output_tokens": 7,
+    "input_tokens_details": {"cached_tokens": 5},
+    "output_tokens_details": {"reasoning_tokens": 2}
+  }
+}`
+			return &http.Response{
+				StatusCode: http.StatusOK,
+				Header:     http.Header{"Content-Type": []string{"application/json"}},
+				Body:       io.NopCloser(strings.NewReader(body)),
+				Request:    r,
+			}, nil
+		}),
+	}
+	defer func() { http.DefaultClient = origClient }()
+
+	eng, err := NewEngine(&settings.StepSettings{
+		API: &settings.APISettings{
+			APIKeys:  map[string]string{"openai-api-key": "test"},
+			BaseUrls: map[string]string{"openai-base-url": "https://example.test/v1"},
+		},
+		Chat: &settings.ChatSettings{
+			Engine: ptr("gpt-5-mini"),
+			Stream: false,
+		},
+	})
+	if err != nil {
+		t.Fatalf("NewEngine: %v", err)
+	}
+
+	sink := &capturingEventSink{}
+	ctx := events.WithEventSinks(context.Background(), sink)
+	turn := &turns.Turn{Blocks: []turns.Block{
+		turns.NewSystemTextBlock("You are a LLM."),
+		turns.NewUserTextBlock("Hello"),
+	}}
+
+	_, err = eng.RunInference(ctx, turn)
+	if err != nil {
+		t.Fatalf("expected no error, got %v", err)
+	}
+
+	var finalEvents int
+	var finalUsage *events.Usage
+	var reasoningTokens int
+	for _, event := range sink.snapshot() {
+		switch e := event.(type) {
+		case *events.EventFinal:
+			finalEvents++
+			finalUsage = e.Metadata().Usage
+			if e.Metadata().Extra != nil {
+				if v, ok := e.Metadata().Extra["reasoning_tokens"].(int); ok {
+					reasoningTokens = v
+				}
+			}
+		}
+	}
+
+	if finalEvents != 1 {
+		t.Fatalf("expected exactly one final event, got %d", finalEvents)
+	}
+	if finalUsage == nil {
+		t.Fatalf("expected final usage metadata")
+	}
+	if finalUsage.InputTokens != 12 || finalUsage.OutputTokens != 7 || finalUsage.CachedTokens != 5 {
+		t.Fatalf("expected usage input=12 output=7 cached=5, got input=%d output=%d cached=%d", finalUsage.InputTokens, finalUsage.OutputTokens, finalUsage.CachedTokens)
+	}
+	if reasoningTokens != 2 {
+		t.Fatalf("expected reasoning_tokens=2 in metadata extra, got %d", reasoningTokens)
+	}
+}
+
+func TestParseUsageTotalsFromResponse_NestedResponseUsage(t *testing.T) {
+	rr := responsesResponse{
+		Response: &responsesResponseNested{
+			Usage: json.RawMessage(`{
+  "input_tokens": 9,
+  "output_tokens": 4,
+  "input_tokens_details": {"cached_tokens": 3},
+  "output_tokens_details": {"reasoning_tokens": 1}
+}`),
+		},
+	}
+
+	totals, ok := parseUsageTotalsFromResponse(rr)
+	if !ok {
+		t.Fatalf("expected usage totals from nested response.usage")
+	}
+	if totals.inputTokens != 9 || totals.outputTokens != 4 || totals.cachedTokens != 3 || totals.reasoningTokens != 1 {
+		t.Fatalf(
+			"unexpected totals: input=%d output=%d cached=%d reasoning=%d",
+			totals.inputTokens,
+			totals.outputTokens,
+			totals.cachedTokens,
+			totals.reasoningTokens,
+		)
+	}
+}
